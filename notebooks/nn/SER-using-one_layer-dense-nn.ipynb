{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvN4E0CjpEAk"
      },
      "source": [
        "### Import dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "LALgcVpUoPAs"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import SGD, Adam\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import time\n",
        "import datetime\n",
        "import numpy as np\n",
        "\n",
        "from ignite.metrics.recall import Recall\n",
        "from ignite.engine import *\n",
        "\n",
        "def eval_step(engine, batch):\n",
        "    return batch\n",
        "\n",
        "default_evaluator = Engine(eval_step)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Prepare Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SER_Dataset(Dataset):\n",
        "    def __init__(self, data_path):\n",
        "        \"\"\"\n",
        "        Аргументы:\n",
        "            data_path -- путь к папке извлеченными признаками\n",
        "        \"\"\"\n",
        "        \n",
        "        self._ids = list([])    # speaker IDs\n",
        "\n",
        "        X = pd.read_csv(\"../../data/2023-11-08/feature_mfcc_34_delta_delta_nfft_4096.csv\", index_col=0)\n",
        "        y = pd.read_csv('../../data/2023-11-08/y_labels_feature_34_mfcc_delta_delta_nfft_4096.csv', index_col=0)\n",
        "        ID= pd.read_csv('../../data/2023-11-08/IDs_feature_mfcc_34_delta_delta_nfft_4096.csv', index_col=0)\n",
        "\n",
        "        # Convert features and labels to numpy arrays\n",
        "        X = X.values\n",
        "        y.replace({'neutral':0, 'calm':1, 'happy':2, 'sad':3, 'angry':4, 'fear':5, 'disgust':6, 'surprised':7}, inplace=True)\n",
        "        y = torch.tensor(y.values)\n",
        "        self.y = torch.nn.functional.one_hot(y, num_classes=8)\n",
        "        self.y = self.y.squeeze(dim=1)\n",
        "        self.y = self.y.float()\n",
        "        self.X = torch.from_numpy(X).float()\n",
        "\n",
        "        self.folds = list(([2, 5, 14, 15, 16], # 0\n",
        "                           [3, 6, 7, 13, 18], # 1\n",
        "                           [10, 11, 12, 19, 20], # 2\n",
        "                           [8, 17, 21, 23, 24], # 3\n",
        "                           [1, 4, 9, 22]))     # 4\n",
        "\n",
        "        self.folds_val = list(([3, 6],\n",
        "                              [10, 11],\n",
        "                              [8, 17],\n",
        "                              [1, 14],\n",
        "                              [2, 5]))\n",
        "\n",
        "        self.X_ids = list(np.array(ID.values).squeeze())        \n",
        "       \n",
        "        # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        # self.X = self.X.to(device)\n",
        "        # self.y = self.y.to(device)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.X.shape[0]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.X[index,:], self.y[index,:]\n",
        "\n",
        "    def get_kth_fold_inds(self, fold_num):\n",
        "        ids_train = list([])\n",
        "        ids_val  = list([])\n",
        "        ids_test = list([])\n",
        "        for i in range(len(self.X_ids)):\n",
        "            # print(self.X_ids[i])\n",
        "            # print(self.folds[fold_num])\n",
        "            if self.X_ids[i] in self.folds[fold_num]:\n",
        "                ids_test.append(i)\n",
        "            elif self.X_ids[i] in self.folds_val[fold_num]:\n",
        "                ids_val.append(i)\n",
        "            else:\n",
        "                ids_train.append(i)\n",
        "        return (ids_train,ids_val, ids_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "## test dataloader\n",
        "SER_data_set = SER_Dataset('')\n",
        "# print(len(SER_data_set))\n",
        "# X,y = SER_data_set[2]\n",
        "# print(X)\n",
        "# print(y)\n",
        "# ids_train,ids_val, ids_test = SER_data_set.get_kth_fold_inds(4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngPcxS0YpDZQ"
      },
      "source": [
        "# Linear NN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Eh785LXNpO6I"
      },
      "outputs": [],
      "source": [
        "class SER_one_layer_NN(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super(SER_one_layer_NN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, num_classes)        \n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        torch.manual_seed(702)\n",
        "        nn.init.xavier_uniform_(self.fc1.weight)        \n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Multiply input by weights and add biases\n",
        "        linear_output = self.fc1(x)\n",
        "        SER_out = torch.nn.functional.softmax(linear_output, dim=0)\n",
        "        return SER_out\n",
        "\n",
        "    # def train_model(self, input_data, target_labels, num_epochs=1000, learning_rate=0.01):\n",
        "    #     # Define the loss function\n",
        "    #     criterion = nn.CrossEntropyLoss()\n",
        "    #     # Define the optimizer\n",
        "    #     optimizer = optim.SGD(self.parameters(), lr=learning_rate)\n",
        "\n",
        "    #     for epoch in range(num_epochs):\n",
        "    #         # Forward pass\n",
        "    #         outputs = self(input_data)\n",
        "\n",
        "    #         # Compute the loss\n",
        "    #         loss = criterion(outputs, target_labels)\n",
        "\n",
        "    #         # Backward pass and optimization\n",
        "    #         optimizer.zero_grad()\n",
        "    #         loss.backward()\n",
        "    #         optimizer.step()\n",
        "\n",
        "    #         # Print the loss every few epochs\n",
        "    #         if epoch % 10 == 0:\n",
        "    #             print(f\"Epoch {epoch}: Loss {loss.item()}\")\n",
        "\n",
        "    # def predict(self, input_data):\n",
        "    #     # Forward pass to get the output logits\n",
        "    #     logits = self(input_data)\n",
        "    #     # Apply softmax to get probabilities\n",
        "    #     probabilities = F.softmax(logits, dim=1)\n",
        "    #     # Get the class label with highest probability\n",
        "    #     predicted_labels = torch.argmax(probabilities, dim=1)\n",
        "    #     return predicted_labels\n",
        "\n",
        "\n",
        "    # def estimate(self, y_true, y_pred):\n",
        "    #   \"\"\"\n",
        "    #   calculates the unweighted average recall across all the classes present\n",
        "    #   in the true labels (y_true)\n",
        "    #   and predicted labels (y_pred) using the confusion matrix.\n",
        "    #   \"\"\"\n",
        "    #   cm = confusion_matrix(y_true, y_pred)\n",
        "    #   num_classes = len(cm)\n",
        "    #   recalls = []\n",
        "    #   for i in range(self.num_classes):\n",
        "    #       true_positives = cm[i, i]\n",
        "    #       actual_positives = sum(cm[i, :])\n",
        "    #       recall = true_positives / actual_positives\n",
        "    #       recalls.append(recall)\n",
        "    #   average_recall = sum(recalls) / num_classes\n",
        "    #   return average_recall"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "X,y = SER_data_set[2]\n",
        "model = SER_one_layer_NN(len(X), y.shape[0])\n",
        "y_out = model(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|██████████| 204/204 [00:00<00:00, 814.58it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-03-17 17:56:22.514139 Epoch 1, Train loss 2.016, Val loss 1.937, UAR_val = 0.359\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|██████████| 204/204 [00:00<00:00, 767.16it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-03-17 17:56:22.805046 Epoch 2, Train loss 1.946, Val loss 1.887, UAR_val = 0.391\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|██████████| 204/204 [00:00<00:00, 767.16it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-03-17 17:56:23.095952 Epoch 3, Train loss 1.903, Val loss 1.862, UAR_val = 0.469\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4: 100%|██████████| 204/204 [00:00<00:00, 694.84it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-03-17 17:56:23.414537 Epoch 4, Train loss 1.850, Val loss 1.854, UAR_val = 0.461\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5: 100%|██████████| 204/204 [00:00<00:00, 793.62it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-03-17 17:56:23.695577 Epoch 5, Train loss 1.850, Val loss 1.837, UAR_val = 0.445\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6: 100%|██████████| 204/204 [00:00<00:00, 800.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-03-17 17:56:23.976487 Epoch 6, Train loss 1.830, Val loss 1.813, UAR_val = 0.531\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7: 100%|██████████| 204/204 [00:00<00:00, 738.69it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-03-17 17:56:24.276645 Epoch 7, Train loss 1.809, Val loss 1.803, UAR_val = 0.531\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8: 100%|██████████| 204/204 [00:00<00:00, 758.61it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-03-17 17:56:24.570551 Epoch 8, Train loss 1.808, Val loss 1.794, UAR_val = 0.539\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9: 100%|██████████| 204/204 [00:00<00:00, 754.54it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-03-17 17:56:24.865910 Epoch 9, Train loss 1.798, Val loss 1.801, UAR_val = 0.531\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10: 100%|██████████| 204/204 [00:00<00:00, 635.72it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-03-17 17:56:25.226794 Epoch 10, Train loss 1.781, Val loss 1.801, UAR_val = 0.500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 11: 100%|██████████| 204/204 [00:00<00:00, 755.49it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-03-17 17:56:25.519809 Epoch 11, Train loss 1.783, Val loss 1.799, UAR_val = 0.562\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 12: 100%|██████████| 204/204 [00:00<00:00, 610.97it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-03-17 17:56:25.882694 Epoch 12, Train loss 1.757, Val loss 1.789, UAR_val = 0.578\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 13: 100%|██████████| 204/204 [00:00<00:00, 787.55it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-03-17 17:56:26.164715 Epoch 13, Train loss 1.767, Val loss 1.795, UAR_val = 0.578\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 14: 100%|██████████| 204/204 [00:00<00:00, 728.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-03-17 17:56:26.469618 Epoch 14, Train loss 1.749, Val loss 1.793, UAR_val = 0.555\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 15: 100%|██████████| 204/204 [00:00<00:00, 800.39it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-03-17 17:56:26.748488 Epoch 15, Train loss 1.737, Val loss 1.786, UAR_val = 0.578\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 16: 100%|██████████| 204/204 [00:00<00:00, 800.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-03-17 17:56:27.033396 Epoch 16, Train loss 1.752, Val loss 1.787, UAR_val = 0.586\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 17: 100%|██████████| 204/204 [00:00<00:00, 809.78it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-03-17 17:56:27.309308 Epoch 17, Train loss 1.752, Val loss 1.788, UAR_val = 0.562\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 18: 100%|██████████| 204/204 [00:00<00:00, 703.40it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-03-17 17:56:27.628434 Epoch 18, Train loss 1.740, Val loss 1.785, UAR_val = 0.555\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 19: 100%|██████████| 204/204 [00:00<00:00, 791.37it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-03-17 17:56:27.911206 Epoch 19, Train loss 1.753, Val loss 1.780, UAR_val = 0.562\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 20: 100%|██████████| 204/204 [00:00<00:00, 797.13it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-03-17 17:56:28.191118 Epoch 20, Train loss 1.739, Val loss 1.772, UAR_val = 0.570\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Ошибка обучения')"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 600x250 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model = SER_one_layer_NN(306,8)\n",
        "n_epochs = 20\n",
        "\n",
        "optimizer = Adam(model.parameters(),lr=5e-4, weight_decay=1e-2) # weight_decay is L2 regularization\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "SER_data_set = SER_Dataset('')\n",
        "\n",
        "inds_train,inds_val,inds_test = SER_data_set.get_kth_fold_inds(0)\n",
        "train_set = torch.utils.data.dataset.Subset(SER_data_set, inds_train)\n",
        "val_set = torch.utils.data.dataset.Subset(SER_data_set, inds_val)\n",
        "# test_set = torch.utils.data.dataset.Subset(SER_data_set, inds_test)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=5,shuffle=True)  # num_workers=2\n",
        "val_loader = torch.utils.data.DataLoader(val_set, batch_size=5, shuffle=False)\n",
        "# test_loader = torch.utils.data.DataLoader(test_set, batch_size=1, shuffle=False) \n",
        "\n",
        "metric = Recall()\n",
        "macro_metric = Recall(average=True)\n",
        "\n",
        "metric.attach(default_evaluator, \"recall\")\n",
        "macro_metric.attach(default_evaluator, \"macro recall\")\n",
        "\n",
        "\n",
        "MODEL_PATH = \"model_backup/model_nn.pt\"\n",
        "loss_train_history = np.ndarray((n_epochs))\n",
        "loss_val_history = np.ndarray((n_epochs))\n",
        "lrs = []\n",
        "best_epoch = -1\n",
        "loss_val_min = 100\n",
        "for epoch in range(1,n_epochs+1):\n",
        "    model.train()\n",
        "    loss_train = 0.0\n",
        "    for X, labels in tqdm(train_loader, desc=f\"Epoch {epoch}\", leave=True): #train_loader:        \n",
        "        outputs = model(X)\n",
        "        outputs = outputs.squeeze(dim=1) # for batch train_loader\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        loss_train += loss.item()\n",
        "    loss_train = loss_train / len(train_loader)\n",
        "\n",
        "    # prediction on val set\n",
        "    loss_val = 0.0\n",
        "    TP_TN_sum = 0\n",
        "    total = 0\n",
        "    model.eval()\n",
        "    val_pred = []\n",
        "    val_true = []\n",
        "    with torch.no_grad():\n",
        "        for X, labels in val_loader:\n",
        "            pred = model(X)\n",
        "            pred = pred.squeeze(dim=1) # for batch train_loader\n",
        "\n",
        "            loss = loss_fn(pred, labels)\n",
        "            loss_val += loss.item()\n",
        "\n",
        "            # _,pred = torch.max(pred, dim=1)\n",
        "            _,labels = torch.max(labels, dim=1)\n",
        "\n",
        "            val_pred.append(pred)\n",
        "            val_true.append(labels)                        \n",
        "\n",
        "        val_pred = torch.cat(val_pred)\n",
        "        val_true = torch.cat(val_true)\n",
        "\n",
        "        default_evaluator.terminate()\n",
        "        state = default_evaluator.run([[val_pred, val_true]])\n",
        "        # print(f\"Recall: {state.metrics['recall']}\")\n",
        "        # print(f\"Macro Recall: {state.metrics['macro recall']}\")\n",
        "\n",
        "        loss_val = loss_val / len(val_loader)\n",
        "\n",
        "        loss_train_history[epoch-1] = loss_train\n",
        "        loss_val_history[epoch-1] = loss_val\n",
        "\n",
        "        if loss_val < loss_val_min:\n",
        "            torch.save(model.state_dict(), MODEL_PATH)\n",
        "            loss_val_min = loss_val\n",
        "            best_epoch = epoch-1\n",
        "        UAR = state.metrics['macro recall']\n",
        "        print(f'{datetime.datetime.now()} Epoch {epoch}, Train loss {loss_train:.3f}, Val loss {loss_val:.3f}, UAR_val = {UAR:.3f}')\n",
        "\n",
        "model.load_state_dict(torch.load(MODEL_PATH))\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(6,2.5))\n",
        "plt.rcParams.update({'font.size': 12})\n",
        "# plt.subplot(1,2,1)\n",
        "plt.plot(range(n_epochs), loss_train_history, label='Обучающий набор')\n",
        "plt.plot(range(n_epochs), loss_val_history, label='Проверочный набор')\n",
        "plt.plot(best_epoch,loss_val_min,color='red', marker='o', linewidth=1, markersize=5)\n",
        "plt.legend()\n",
        "plt.xlabel('Эпоха')\n",
        "plt.ylabel('Ошибка обучения')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Train loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def training_loop(n_epochs, optimizer, lr_scheduler, model, loss_fn, train_loader, val_loader):\n",
        "    MODEL_PATH = \"model_backup/model_nn.pt\"\n",
        "    loss_train_history = np.ndarray((n_epochs))\n",
        "    loss_val_history = np.ndarray((n_epochs))\n",
        "    lrs = []\n",
        "    best_epoch = -1\n",
        "    loss_val_min = 100\n",
        "    for epoch in range(1,n_epochs+1):\n",
        "        model.train()\n",
        "        loss_train = 0.0\n",
        "        for specs, labels in tqdm(train_loader, desc=f\"Epoch {epoch}\", leave=True): #train_loader:\n",
        "            specs = specs.float()\n",
        "            outputs = model(specs)\n",
        "            outputs = outputs.squeeze(dim=1) # for batch train_loader\n",
        "            # print('output:',outputs)\n",
        "            # print('labels:',labels)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            loss_train += loss.item()\n",
        "        loss_train = loss_train / len(train_loader)\n",
        "\n",
        "        # prediction on val set\n",
        "        loss_val = 0.0\n",
        "        TP_TN_sum = 0\n",
        "        total = 0\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for spec, label in val_loader:\n",
        "                spec = spec.float()\n",
        "                pred = model(spec)\n",
        "                pred = torch.squeeze(pred,1)\n",
        "\n",
        "                loss = loss_fn(pred, label)\n",
        "                loss_val += loss.item()\n",
        "\n",
        "                _,pred = torch.max(pred, dim=1)\n",
        "                _,label = torch.max(label, dim=1)\n",
        "\n",
        "                total += len(label)\n",
        "\n",
        "                TP_TN_sum = TP_TN_sum + torch.sum(label==pred)\n",
        "\n",
        "        acc_val = TP_TN_sum/total\n",
        "        loss_val = loss_val / len(val_loader)\n",
        "\n",
        "        loss_train_history[epoch-1] = loss_train\n",
        "        loss_val_history[epoch-1] = loss_val\n",
        "\n",
        "        if loss_val < loss_val_min:\n",
        "            torch.save(model.state_dict(), MODEL_PATH)\n",
        "            loss_val_min = loss_val\n",
        "            best_epoch = epoch-1\n",
        "\n",
        "        # if epoch==1 or epoch%2==0:\n",
        "        print(f'{datetime.datetime.now()} Epoch {epoch}, Train loss {loss_train:.3f}, Val loss {loss_val:.3f}, Acc_val = {acc_val:.2f}')\n",
        "\n",
        "        lrs.append(optimizer.param_groups[0][\"lr\"])\n",
        "        lr_scheduler.step()\n",
        "\n",
        "    # model = torch.load(MODEL_PATH)\n",
        "    model.load_state_dict(torch.load(MODEL_PATH))\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(6,2.5))\n",
        "    plt.rcParams.update({'font.size': 12})\n",
        "    # plt.subplot(1,2,1)\n",
        "    plt.plot(range(n_epochs), loss_train_history, label='Обучающий набор')\n",
        "    plt.plot(range(n_epochs), loss_val_history, label='Проверочный набор')\n",
        "    plt.plot(best_epoch,loss_val_min,color='red', marker='o', linewidth=1, markersize=5)\n",
        "    plt.legend()\n",
        "    plt.xlabel('Эпоха')\n",
        "    plt.ylabel('Ошибка обучения')\n",
        "    # plt.subplot(1,2,2)\n",
        "    # plt.plot(range(len(lrs)),lrs)\n",
        "    # plt.xlabel('step')\n",
        "    # plt.ylabel('lr')\n",
        "    plt.show()\n",
        "    # Saving results\n",
        "    # image_name = f'figures/loss_NN.jpg'\n",
        "    # fig.savefig(image_name, format='jpg', dpi=110, bbox_inches='tight', pad_inches = 0.2)\n",
        "    return (loss_train_history,loss_val_history,loss_val_min,best_epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7DIuHzeFIDa"
      },
      "source": [
        "### Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 194,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zn8o-FxFE9dv",
        "outputId": "740fa71c-5241-4a18-bcfe-1f7b922614a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[7]\n",
            " [2]\n",
            " [1]\n",
            " ...\n",
            " [7]\n",
            " [6]\n",
            " [5]]\n",
            "Shape of x_tensor: torch.Size([1440, 306])\n",
            "Shape of y_tensor: torch.Size([1440])\n"
          ]
        }
      ],
      "source": [
        "# Load data from CSV files using pandas\n",
        "features_df = pd.read_csv('/content/feature_mfcc_34_delta_delta_nfft_8192.csv', index_col=0)\n",
        "labels_df = pd.read_csv('/content/y_labels_feature_34_mfcc_delta_delta_nfft_8192.csv', index_col=0)\n",
        "\n",
        "# Convert features and labels to numpy arrays\n",
        "features = features_df.values\n",
        "labels_df.replace({'neutral':0, 'calm':1, 'happy':2, 'sad':3, 'angry':4, 'fear':5, 'disgust':6, 'surprised':7}, inplace=True)\n",
        "labels = labels_df.values\n",
        "print(labels)\n",
        "\n",
        "# Convert numpy arrays to PyTorch tensors\n",
        "x_tensor = torch.tensor(features, dtype=torch.float32)\n",
        "y_tensor = torch.tensor(labels, dtype=torch.long).squeeze()  # Assuming labels are integers (long tensor)\n",
        "\n",
        "# Print shape of tensors\n",
        "print(\"Shape of x_tensor:\", x_tensor.shape)\n",
        "print(\"Shape of y_tensor:\", y_tensor.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 201,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ori_ITnsFCPP",
        "outputId": "eb67bf51-c2f6-48e4-9d8e-4ac26098c0b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of x_train: torch.Size([1007, 306])\n",
            "Shape of y_train: torch.Size([1007])\n",
            "Shape of x_test: torch.Size([433, 306])\n",
            "Shape of y_test: torch.Size([433])\n"
          ]
        }
      ],
      "source": [
        "train_ratio = 0.7  # 80% of the data for training, 20% for testing\n",
        "# Calculate the number of samples for training and testing\n",
        "train_size = int(train_ratio * len(x_tensor))\n",
        "test_size = len(x_tensor) - train_size\n",
        "\n",
        "# Shuffle the indices of the samples\n",
        "indices = torch.randperm(len(x_tensor))\n",
        "\n",
        "# Split the indices into training and testing sets\n",
        "train_indices = indices[:train_size]\n",
        "test_indices = indices[train_size:]\n",
        "\n",
        "# Use the indices to extract the samples for training and testing\n",
        "x_train, y_train = x_tensor[train_indices], y_tensor[train_indices]\n",
        "x_test, y_test = x_tensor[test_indices], y_tensor[test_indices]\n",
        "\n",
        "# Print the shapes of the training and testing sets\n",
        "print(\"Shape of x_train:\", x_train.shape)\n",
        "print(\"Shape of y_train:\", y_train.shape)\n",
        "print(\"Shape of x_test:\", x_test.shape)\n",
        "print(\"Shape of y_test:\", y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 204,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bXntyjvzFNx_",
        "outputId": "fb9a94be-58ff-428f-c3c3-eb6a5b538308"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0: Loss 22.201038360595703\n",
            "Epoch 10: Loss 12.007477760314941\n",
            "Epoch 20: Loss 9.196626663208008\n",
            "Epoch 30: Loss 7.431214332580566\n",
            "Epoch 40: Loss 6.15256404876709\n",
            "Epoch 50: Loss 5.200140476226807\n",
            "Epoch 60: Loss 4.501382350921631\n",
            "Epoch 70: Loss 3.9571022987365723\n",
            "Epoch 80: Loss 3.51375412940979\n",
            "Epoch 90: Loss 3.1484124660491943\n",
            "Epoch 100: Loss 2.842606544494629\n",
            "Epoch 110: Loss 2.582691192626953\n",
            "Epoch 120: Loss 2.359424114227295\n",
            "Epoch 130: Loss 2.167097806930542\n",
            "Epoch 140: Loss 2.000316619873047\n",
            "Epoch 150: Loss 1.853821039199829\n",
            "Epoch 160: Loss 1.7234177589416504\n",
            "Epoch 170: Loss 1.6062692403793335\n",
            "Epoch 180: Loss 1.5004349946975708\n",
            "Epoch 190: Loss 1.4046496152877808\n",
            "Epoch 200: Loss 1.3181558847427368\n",
            "Epoch 210: Loss 1.2400637865066528\n",
            "Epoch 220: Loss 1.169301152229309\n",
            "Epoch 230: Loss 1.1049368381500244\n",
            "Epoch 240: Loss 1.046176552772522\n",
            "Epoch 250: Loss 0.9923300743103027\n",
            "Epoch 260: Loss 0.9428204298019409\n",
            "Epoch 270: Loss 0.8972203135490417\n",
            "Epoch 280: Loss 0.8552097082138062\n",
            "Epoch 290: Loss 0.8164157271385193\n",
            "Epoch 300: Loss 0.7804450988769531\n",
            "Epoch 310: Loss 0.746967077255249\n",
            "Epoch 320: Loss 0.7157087326049805\n",
            "Epoch 330: Loss 0.6864404678344727\n",
            "Epoch 340: Loss 0.6589691042900085\n",
            "Epoch 350: Loss 0.6331303715705872\n",
            "Epoch 360: Loss 0.6087819337844849\n",
            "Epoch 370: Loss 0.585800051689148\n",
            "Epoch 380: Loss 0.5640761852264404\n",
            "Epoch 390: Loss 0.5435153841972351\n",
            "Epoch 400: Loss 0.5240335464477539\n",
            "Epoch 410: Loss 0.5055558085441589\n",
            "Epoch 420: Loss 0.4880148768424988\n",
            "Epoch 430: Loss 0.47135061025619507\n",
            "Epoch 440: Loss 0.4555092751979828\n",
            "Epoch 450: Loss 0.4404431879520416\n",
            "Epoch 460: Loss 0.4261089861392975\n",
            "Epoch 470: Loss 0.41246676445007324\n",
            "Epoch 480: Loss 0.3994787037372589\n",
            "Epoch 490: Loss 0.3871079683303833\n",
            "Epoch 500: Loss 0.3753190338611603\n",
            "Epoch 510: Loss 0.3640778958797455\n",
            "Epoch 520: Loss 0.353352427482605\n",
            "Epoch 530: Loss 0.34311315417289734\n",
            "Epoch 540: Loss 0.3333326578140259\n",
            "Epoch 550: Loss 0.3239850699901581\n",
            "Epoch 560: Loss 0.3150465786457062\n",
            "Epoch 570: Loss 0.30649426579475403\n",
            "Epoch 580: Loss 0.2983072102069855\n",
            "Epoch 590: Loss 0.2904658615589142\n",
            "Epoch 600: Loss 0.28295227885246277\n",
            "Epoch 610: Loss 0.27575013041496277\n",
            "Epoch 620: Loss 0.2688443064689636\n",
            "Epoch 630: Loss 0.2622212767601013\n",
            "Epoch 640: Loss 0.25586792826652527\n",
            "Epoch 650: Loss 0.24977241456508636\n",
            "Epoch 660: Loss 0.24392306804656982\n",
            "Epoch 670: Loss 0.23830899596214294\n",
            "Epoch 680: Loss 0.2329196035861969\n",
            "Epoch 690: Loss 0.22774450480937958\n",
            "Epoch 700: Loss 0.22277359664440155\n",
            "Epoch 710: Loss 0.21799726784229279\n",
            "Epoch 720: Loss 0.21340619027614594\n",
            "Epoch 730: Loss 0.20899133384227753\n",
            "Epoch 740: Loss 0.20474404096603394\n",
            "Epoch 750: Loss 0.20065602660179138\n",
            "Epoch 760: Loss 0.19671940803527832\n",
            "Epoch 770: Loss 0.1929267793893814\n",
            "Epoch 780: Loss 0.18927101790905\n",
            "Epoch 790: Loss 0.18574541807174683\n",
            "Epoch 800: Loss 0.18234354257583618\n",
            "Epoch 810: Loss 0.17905952036380768\n",
            "Epoch 820: Loss 0.17588762938976288\n",
            "Epoch 830: Loss 0.1728225201368332\n",
            "Epoch 840: Loss 0.1698591262102127\n",
            "Epoch 850: Loss 0.16699287295341492\n",
            "Epoch 860: Loss 0.16421915590763092\n",
            "Epoch 870: Loss 0.16153383255004883\n",
            "Epoch 880: Loss 0.15893302857875824\n",
            "Epoch 890: Loss 0.15641294419765472\n",
            "Epoch 900: Loss 0.15396995842456818\n",
            "Epoch 910: Loss 0.15160086750984192\n",
            "Epoch 920: Loss 0.1493024080991745\n",
            "Epoch 930: Loss 0.14707155525684357\n",
            "Epoch 940: Loss 0.14490540325641632\n",
            "Epoch 950: Loss 0.14280109107494354\n",
            "Epoch 960: Loss 0.14075607061386108\n",
            "Epoch 970: Loss 0.13876786828041077\n",
            "Epoch 980: Loss 0.1368338167667389\n",
            "Epoch 990: Loss 0.13495180010795593\n",
            "Epoch 1000: Loss 0.13311952352523804\n",
            "Epoch 1010: Loss 0.13133493065834045\n",
            "Epoch 1020: Loss 0.12959598004817963\n",
            "Epoch 1030: Loss 0.1279008537530899\n",
            "Epoch 1040: Loss 0.12624774873256683\n",
            "Epoch 1050: Loss 0.12463489919900894\n",
            "Epoch 1060: Loss 0.12306083738803864\n",
            "Epoch 1070: Loss 0.12152394652366638\n",
            "Epoch 1080: Loss 0.12002284079790115\n",
            "Epoch 1090: Loss 0.1185561865568161\n",
            "Epoch 1100: Loss 0.11712268739938736\n",
            "Epoch 1110: Loss 0.11572112143039703\n",
            "Epoch 1120: Loss 0.11435046792030334\n",
            "Epoch 1130: Loss 0.11300954967737198\n",
            "Epoch 1140: Loss 0.11169735342264175\n",
            "Epoch 1150: Loss 0.11041299998760223\n",
            "Epoch 1160: Loss 0.10915551334619522\n",
            "Epoch 1170: Loss 0.10792408138513565\n",
            "Epoch 1180: Loss 0.10671792924404144\n",
            "Epoch 1190: Loss 0.10553622245788574\n",
            "Epoch 1200: Loss 0.10437828302383423\n",
            "Epoch 1210: Loss 0.103243388235569\n",
            "Epoch 1220: Loss 0.10213097929954529\n",
            "Epoch 1230: Loss 0.10104035586118698\n",
            "Epoch 1240: Loss 0.0999709963798523\n",
            "Epoch 1250: Loss 0.0989222601056099\n",
            "Epoch 1260: Loss 0.09789369255304337\n",
            "Epoch 1270: Loss 0.09688476473093033\n",
            "Epoch 1280: Loss 0.09589500725269318\n",
            "Epoch 1290: Loss 0.0949239432811737\n",
            "Epoch 1300: Loss 0.09397109597921371\n",
            "Epoch 1310: Loss 0.09303612262010574\n",
            "Epoch 1320: Loss 0.09211849421262741\n",
            "Epoch 1330: Loss 0.09121788293123245\n",
            "Epoch 1340: Loss 0.09033390879631042\n",
            "Epoch 1350: Loss 0.08946617692708969\n",
            "Epoch 1360: Loss 0.08861426264047623\n",
            "Epoch 1370: Loss 0.08777789026498795\n",
            "Epoch 1380: Loss 0.08695666491985321\n",
            "Epoch 1390: Loss 0.08615026623010635\n",
            "Epoch 1400: Loss 0.08535830676555634\n",
            "Epoch 1410: Loss 0.08458051830530167\n",
            "Epoch 1420: Loss 0.08381655812263489\n",
            "Epoch 1430: Loss 0.08306612074375153\n",
            "Epoch 1440: Loss 0.08232884109020233\n",
            "Epoch 1450: Loss 0.08160451799631119\n",
            "Epoch 1460: Loss 0.08089274913072586\n",
            "Epoch 1470: Loss 0.08019335567951202\n",
            "Epoch 1480: Loss 0.07950595766305923\n",
            "Epoch 1490: Loss 0.07883031666278839\n",
            "Epoch 1500: Loss 0.07816620916128159\n",
            "Epoch 1510: Loss 0.07751332223415375\n",
            "Epoch 1520: Loss 0.07687141001224518\n",
            "Epoch 1530: Loss 0.07624022662639618\n",
            "Epoch 1540: Loss 0.07561954855918884\n",
            "Epoch 1550: Loss 0.07500911504030228\n",
            "Epoch 1560: Loss 0.07440873980522156\n",
            "Epoch 1570: Loss 0.0738181546330452\n",
            "Epoch 1580: Loss 0.07323715090751648\n",
            "Epoch 1590: Loss 0.07266553491353989\n",
            "Epoch 1600: Loss 0.07210307568311691\n",
            "Epoch 1610: Loss 0.07154960185289383\n",
            "Epoch 1620: Loss 0.07100490480661392\n",
            "Epoch 1630: Loss 0.07046879827976227\n",
            "Epoch 1640: Loss 0.06994106620550156\n",
            "Epoch 1650: Loss 0.06942160427570343\n",
            "Epoch 1660: Loss 0.06891018897294998\n",
            "Epoch 1670: Loss 0.06840663403272629\n",
            "Epoch 1680: Loss 0.0679108053445816\n",
            "Epoch 1690: Loss 0.0674225240945816\n",
            "Epoch 1700: Loss 0.06694165617227554\n",
            "Epoch 1710: Loss 0.06646797806024551\n",
            "Epoch 1720: Loss 0.06600143015384674\n",
            "Epoch 1730: Loss 0.0655418187379837\n",
            "Epoch 1740: Loss 0.06508897989988327\n",
            "Epoch 1750: Loss 0.06464281678199768\n",
            "Epoch 1760: Loss 0.0642031580209732\n",
            "Epoch 1770: Loss 0.06376990675926208\n",
            "Epoch 1780: Loss 0.06334288418292999\n",
            "Epoch 1790: Loss 0.06292200088500977\n",
            "Epoch 1800: Loss 0.06250711530447006\n",
            "Epoch 1810: Loss 0.06209808960556984\n",
            "Epoch 1820: Loss 0.06169482693076134\n",
            "Epoch 1830: Loss 0.06129719316959381\n",
            "Epoch 1840: Loss 0.060905102640390396\n",
            "Epoch 1850: Loss 0.06051843240857124\n",
            "Epoch 1860: Loss 0.0601370595395565\n",
            "Epoch 1870: Loss 0.05976087227463722\n",
            "Epoch 1880: Loss 0.059389784932136536\n",
            "Epoch 1890: Loss 0.059023719280958176\n",
            "Epoch 1900: Loss 0.0586625300347805\n",
            "Epoch 1910: Loss 0.058306120336055756\n",
            "Epoch 1920: Loss 0.05795442312955856\n",
            "Epoch 1930: Loss 0.05760734900832176\n",
            "Epoch 1940: Loss 0.057264797389507294\n",
            "Epoch 1950: Loss 0.05692664161324501\n",
            "Epoch 1960: Loss 0.05659287050366402\n",
            "Epoch 1970: Loss 0.05626334622502327\n",
            "Epoch 1980: Loss 0.055938005447387695\n",
            "Epoch 1990: Loss 0.055616769939661026\n",
            "Epoch 2000: Loss 0.0552995428442955\n",
            "Epoch 2010: Loss 0.05498626083135605\n",
            "Epoch 2020: Loss 0.05467686429619789\n",
            "Epoch 2030: Loss 0.05437127500772476\n",
            "Epoch 2040: Loss 0.0540693923830986\n",
            "Epoch 2050: Loss 0.05377116799354553\n",
            "Epoch 2060: Loss 0.05347654968500137\n",
            "Epoch 2070: Loss 0.05318543687462807\n",
            "Epoch 2080: Loss 0.052897822111845016\n",
            "Epoch 2090: Loss 0.05261356383562088\n",
            "Epoch 2100: Loss 0.052332669496536255\n",
            "Epoch 2110: Loss 0.052055031061172485\n",
            "Epoch 2120: Loss 0.05178060382604599\n",
            "Epoch 2130: Loss 0.051509369164705276\n",
            "Epoch 2140: Loss 0.05124121159315109\n",
            "Epoch 2150: Loss 0.050976090133190155\n",
            "Epoch 2160: Loss 0.05071399360895157\n",
            "Epoch 2170: Loss 0.05045482516288757\n",
            "Epoch 2180: Loss 0.050198569893836975\n",
            "Epoch 2190: Loss 0.049945130944252014\n",
            "Epoch 2200: Loss 0.049694497138261795\n",
            "Epoch 2210: Loss 0.04944662004709244\n",
            "Epoch 2220: Loss 0.04920143634080887\n",
            "Epoch 2230: Loss 0.048958901315927505\n",
            "Epoch 2240: Loss 0.04871899262070656\n",
            "Epoch 2250: Loss 0.048481620848178864\n",
            "Epoch 2260: Loss 0.048246800899505615\n",
            "Epoch 2270: Loss 0.04801445081830025\n",
            "Epoch 2280: Loss 0.047784533351659775\n",
            "Epoch 2290: Loss 0.0475570484995842\n",
            "Epoch 2300: Loss 0.04733188822865486\n",
            "Epoch 2310: Loss 0.04710905998945236\n",
            "Epoch 2320: Loss 0.04688854143023491\n",
            "Epoch 2330: Loss 0.04667026177048683\n",
            "Epoch 2340: Loss 0.04645421728491783\n",
            "Epoch 2350: Loss 0.04624031111598015\n",
            "Epoch 2360: Loss 0.04602858051657677\n",
            "Epoch 2370: Loss 0.04581896960735321\n",
            "Epoch 2380: Loss 0.0456114262342453\n",
            "Epoch 2390: Loss 0.04540592059493065\n",
            "Epoch 2400: Loss 0.04520244151353836\n",
            "Epoch 2410: Loss 0.045000966638326645\n",
            "Epoch 2420: Loss 0.04480142891407013\n",
            "Epoch 2430: Loss 0.04460383206605911\n",
            "Epoch 2440: Loss 0.04440811276435852\n",
            "Epoch 2450: Loss 0.04421427100896835\n",
            "Epoch 2460: Loss 0.044022269546985626\n",
            "Epoch 2470: Loss 0.04383208602666855\n",
            "Epoch 2480: Loss 0.04364369809627533\n",
            "Epoch 2490: Loss 0.04345707595348358\n",
            "Epoch 2500: Loss 0.043272171169519424\n",
            "Epoch 2510: Loss 0.04308899864554405\n",
            "Epoch 2520: Loss 0.04290749505162239\n",
            "Epoch 2530: Loss 0.04272765293717384\n",
            "Epoch 2540: Loss 0.04254944995045662\n",
            "Epoch 2550: Loss 0.04237288236618042\n",
            "Epoch 2560: Loss 0.04219789057970047\n",
            "Epoch 2570: Loss 0.04202446714043617\n",
            "Epoch 2580: Loss 0.04185261204838753\n",
            "Epoch 2590: Loss 0.041682250797748566\n",
            "Epoch 2600: Loss 0.041513413190841675\n",
            "Epoch 2610: Loss 0.041346076875925064\n",
            "Epoch 2620: Loss 0.04118017479777336\n",
            "Epoch 2630: Loss 0.04101574420928955\n",
            "Epoch 2640: Loss 0.040852732956409454\n",
            "Epoch 2650: Loss 0.040691111236810684\n",
            "Epoch 2660: Loss 0.040530893951654434\n",
            "Epoch 2670: Loss 0.04037204757332802\n",
            "Epoch 2680: Loss 0.040214549750089645\n",
            "Epoch 2690: Loss 0.04005837440490723\n",
            "Epoch 2700: Loss 0.03990351781249046\n",
            "Epoch 2710: Loss 0.03974996507167816\n",
            "Epoch 2720: Loss 0.03959767892956734\n",
            "Epoch 2730: Loss 0.039446666836738586\n",
            "Epoch 2740: Loss 0.03929692134261131\n",
            "Epoch 2750: Loss 0.039148397743701935\n",
            "Epoch 2760: Loss 0.03900106996297836\n",
            "Epoch 2770: Loss 0.03885497897863388\n",
            "Epoch 2780: Loss 0.03871005028486252\n",
            "Epoch 2790: Loss 0.03856627643108368\n",
            "Epoch 2800: Loss 0.038423679769039154\n",
            "Epoch 2810: Loss 0.03828222677111626\n",
            "Epoch 2820: Loss 0.03814190253615379\n",
            "Epoch 2830: Loss 0.03800269588828087\n",
            "Epoch 2840: Loss 0.0378645695745945\n",
            "Epoch 2850: Loss 0.03772754222154617\n",
            "Epoch 2860: Loss 0.0375915952026844\n",
            "Epoch 2870: Loss 0.0374566875398159\n",
            "Epoch 2880: Loss 0.037322841584682465\n",
            "Epoch 2890: Loss 0.0371900275349617\n",
            "Epoch 2900: Loss 0.03705823794007301\n",
            "Epoch 2910: Loss 0.03692745044827461\n",
            "Epoch 2920: Loss 0.0367976650595665\n",
            "Epoch 2930: Loss 0.03666886314749718\n",
            "Epoch 2940: Loss 0.03654104098677635\n",
            "Epoch 2950: Loss 0.03641418367624283\n",
            "Epoch 2960: Loss 0.03628827631473541\n",
            "Epoch 2970: Loss 0.036163315176963806\n",
            "Epoch 2980: Loss 0.036039259284734726\n",
            "Epoch 2990: Loss 0.03591614589095116\n",
            "Epoch 3000: Loss 0.035793937742710114\n",
            "Epoch 3010: Loss 0.0356726236641407\n",
            "Epoch 3020: Loss 0.03555219992995262\n",
            "Epoch 3030: Loss 0.03543265163898468\n",
            "Epoch 3040: Loss 0.035313963890075684\n",
            "Epoch 3050: Loss 0.03519613668322563\n",
            "Epoch 3060: Loss 0.03507916256785393\n",
            "Epoch 3070: Loss 0.03496303781867027\n",
            "Epoch 3080: Loss 0.03484771028161049\n",
            "Epoch 3090: Loss 0.03473324328660965\n",
            "Epoch 3100: Loss 0.03461955860257149\n",
            "Epoch 3110: Loss 0.03450668603181839\n",
            "Epoch 3120: Loss 0.03439461439847946\n",
            "Epoch 3130: Loss 0.03428332135081291\n",
            "Epoch 3140: Loss 0.03417280688881874\n",
            "Epoch 3150: Loss 0.03406304493546486\n",
            "Epoch 3160: Loss 0.03395405039191246\n",
            "Epoch 3170: Loss 0.03384581580758095\n",
            "Epoch 3180: Loss 0.033738311380147934\n",
            "Epoch 3190: Loss 0.03363155573606491\n",
            "Epoch 3200: Loss 0.03352552652359009\n",
            "Epoch 3210: Loss 0.033420197665691376\n",
            "Epoch 3220: Loss 0.033315595239400864\n",
            "Epoch 3230: Loss 0.03321170434355736\n",
            "Epoch 3240: Loss 0.03310851380228996\n",
            "Epoch 3250: Loss 0.033006004989147186\n",
            "Epoch 3260: Loss 0.03290417417883873\n",
            "Epoch 3270: Loss 0.032803021371364594\n",
            "Epoch 3280: Loss 0.03270253911614418\n",
            "Epoch 3290: Loss 0.03260272741317749\n",
            "Epoch 3300: Loss 0.032503560185432434\n",
            "Epoch 3310: Loss 0.03240503743290901\n",
            "Epoch 3320: Loss 0.03230716660618782\n",
            "Epoch 3330: Loss 0.03220992535352707\n",
            "Epoch 3340: Loss 0.03211332485079765\n",
            "Epoch 3350: Loss 0.032017339020967484\n",
            "Epoch 3360: Loss 0.03192196041345596\n",
            "Epoch 3370: Loss 0.03182720020413399\n",
            "Epoch 3380: Loss 0.03173304349184036\n",
            "Epoch 3390: Loss 0.03163949400186539\n",
            "Epoch 3400: Loss 0.031546544283628464\n",
            "Epoch 3410: Loss 0.031454142183065414\n",
            "Epoch 3420: Loss 0.03136235848069191\n",
            "Epoch 3430: Loss 0.031271129846572876\n",
            "Epoch 3440: Loss 0.031180480495095253\n",
            "Epoch 3450: Loss 0.031090401113033295\n",
            "Epoch 3460: Loss 0.03100086934864521\n",
            "Epoch 3470: Loss 0.030911898240447044\n",
            "Epoch 3480: Loss 0.0308234766125679\n",
            "Epoch 3490: Loss 0.030735572800040245\n",
            "Epoch 3500: Loss 0.030648227781057358\n",
            "Epoch 3510: Loss 0.0305614210665226\n",
            "Epoch 3520: Loss 0.030475124716758728\n",
            "Epoch 3530: Loss 0.030389362946152687\n",
            "Epoch 3540: Loss 0.030304132029414177\n",
            "Epoch 3550: Loss 0.030219389125704765\n",
            "Epoch 3560: Loss 0.030135171487927437\n",
            "Epoch 3570: Loss 0.03005143627524376\n",
            "Epoch 3580: Loss 0.02996821515262127\n",
            "Epoch 3590: Loss 0.029885487630963326\n",
            "Epoch 3600: Loss 0.029803236946463585\n",
            "Epoch 3610: Loss 0.029721474274992943\n",
            "Epoch 3620: Loss 0.029640205204486847\n",
            "Epoch 3630: Loss 0.029559407383203506\n",
            "Epoch 3640: Loss 0.02947908826172352\n",
            "Epoch 3650: Loss 0.029399223625659943\n",
            "Epoch 3660: Loss 0.029319820925593376\n",
            "Epoch 3670: Loss 0.02924088016152382\n",
            "Epoch 3680: Loss 0.02916240505874157\n",
            "Epoch 3690: Loss 0.029084382578730583\n",
            "Epoch 3700: Loss 0.029006803408265114\n",
            "Epoch 3710: Loss 0.02892964705824852\n",
            "Epoch 3720: Loss 0.02885294333100319\n",
            "Epoch 3730: Loss 0.028776677325367928\n",
            "Epoch 3740: Loss 0.02870084159076214\n",
            "Epoch 3750: Loss 0.028625430539250374\n",
            "Epoch 3760: Loss 0.028550444170832634\n",
            "Epoch 3770: Loss 0.028475891798734665\n",
            "Epoch 3780: Loss 0.02840173989534378\n",
            "Epoch 3790: Loss 0.028328005224466324\n",
            "Epoch 3800: Loss 0.028254685923457146\n",
            "Epoch 3810: Loss 0.028181757777929306\n",
            "Epoch 3820: Loss 0.028109239414334297\n",
            "Epoch 3830: Loss 0.02803712897002697\n",
            "Epoch 3840: Loss 0.027965398505330086\n",
            "Epoch 3850: Loss 0.027894072234630585\n",
            "Epoch 3860: Loss 0.027823135256767273\n",
            "Epoch 3870: Loss 0.027752574533224106\n",
            "Epoch 3880: Loss 0.027682404965162277\n",
            "Epoch 3890: Loss 0.027612613514065742\n",
            "Epoch 3900: Loss 0.027543192729353905\n",
            "Epoch 3910: Loss 0.02747415006160736\n",
            "Epoch 3920: Loss 0.027405472472310066\n",
            "Epoch 3930: Loss 0.02733715809881687\n",
            "Epoch 3940: Loss 0.027269216254353523\n",
            "Epoch 3950: Loss 0.027201630175113678\n",
            "Epoch 3960: Loss 0.027134403586387634\n",
            "Epoch 3970: Loss 0.027067527174949646\n",
            "Epoch 3980: Loss 0.027001002803444862\n",
            "Epoch 3990: Loss 0.02693483792245388\n",
            "Epoch 4000: Loss 0.026869017630815506\n",
            "Epoch 4010: Loss 0.026803532615303993\n",
            "Epoch 4020: Loss 0.026738394051790237\n",
            "Epoch 4030: Loss 0.026673587039113045\n",
            "Epoch 4040: Loss 0.02660912089049816\n",
            "Epoch 4050: Loss 0.026544976979494095\n",
            "Epoch 4060: Loss 0.026481173932552338\n",
            "Epoch 4070: Loss 0.0264176893979311\n",
            "Epoch 4080: Loss 0.02635454200208187\n",
            "Epoch 4090: Loss 0.026291705667972565\n",
            "Epoch 4100: Loss 0.026229191571474075\n",
            "Epoch 4110: Loss 0.0261670034378767\n",
            "Epoch 4120: Loss 0.026105113327503204\n",
            "Epoch 4130: Loss 0.026043551042675972\n",
            "Epoch 4140: Loss 0.02598228119313717\n",
            "Epoch 4150: Loss 0.025921335443854332\n",
            "Epoch 4160: Loss 0.025860683992505074\n",
            "Epoch 4170: Loss 0.025800343602895737\n",
            "Epoch 4180: Loss 0.025740299373865128\n",
            "Epoch 4190: Loss 0.025680555030703545\n",
            "Epoch 4200: Loss 0.025621090084314346\n",
            "Epoch 4210: Loss 0.02556193247437477\n",
            "Epoch 4220: Loss 0.025503071025013924\n",
            "Epoch 4230: Loss 0.02544448897242546\n",
            "Epoch 4240: Loss 0.02538619004189968\n",
            "Epoch 4250: Loss 0.02532818913459778\n",
            "Epoch 4260: Loss 0.02527046576142311\n",
            "Epoch 4270: Loss 0.025213023647665977\n",
            "Epoch 4280: Loss 0.02515585906803608\n",
            "Epoch 4290: Loss 0.02509896270930767\n",
            "Epoch 4300: Loss 0.025042355060577393\n",
            "Epoch 4310: Loss 0.02498599886894226\n",
            "Epoch 4320: Loss 0.024929935112595558\n",
            "Epoch 4330: Loss 0.024874120950698853\n",
            "Epoch 4340: Loss 0.024818575009703636\n",
            "Epoch 4350: Loss 0.024763301014900208\n",
            "Epoch 4360: Loss 0.024708282202482224\n",
            "Epoch 4370: Loss 0.02465352788567543\n",
            "Epoch 4380: Loss 0.02459903620183468\n",
            "Epoch 4390: Loss 0.024544792249798775\n",
            "Epoch 4400: Loss 0.024490807205438614\n",
            "Epoch 4410: Loss 0.024437077343463898\n",
            "Epoch 4420: Loss 0.02438359521329403\n",
            "Epoch 4430: Loss 0.02433035336434841\n",
            "Epoch 4440: Loss 0.024277379736304283\n",
            "Epoch 4450: Loss 0.024224629625678062\n",
            "Epoch 4460: Loss 0.024172136560082436\n",
            "Epoch 4470: Loss 0.02411988563835621\n",
            "Epoch 4480: Loss 0.024067869409918785\n",
            "Epoch 4490: Loss 0.024016086012125015\n",
            "Epoch 4500: Loss 0.023964546620845795\n",
            "Epoch 4510: Loss 0.023913243785500526\n",
            "Epoch 4520: Loss 0.02386217936873436\n",
            "Epoch 4530: Loss 0.0238113421946764\n",
            "Epoch 4540: Loss 0.023760735988616943\n",
            "Epoch 4550: Loss 0.023710351437330246\n",
            "Epoch 4560: Loss 0.023660210892558098\n",
            "Epoch 4570: Loss 0.023610275238752365\n",
            "Epoch 4580: Loss 0.023560572415590286\n",
            "Epoch 4590: Loss 0.023511085659265518\n",
            "Epoch 4600: Loss 0.023461826145648956\n",
            "Epoch 4610: Loss 0.023412780836224556\n",
            "Epoch 4620: Loss 0.02336396463215351\n",
            "Epoch 4630: Loss 0.023315351456403732\n",
            "Epoch 4640: Loss 0.023266959935426712\n",
            "Epoch 4650: Loss 0.023218786343932152\n",
            "Epoch 4660: Loss 0.02317081019282341\n",
            "Epoch 4670: Loss 0.023123061284422874\n",
            "Epoch 4680: Loss 0.02307550609111786\n",
            "Epoch 4690: Loss 0.02302817441523075\n",
            "Epoch 4700: Loss 0.022981038317084312\n",
            "Epoch 4710: Loss 0.022934118285775185\n",
            "Epoch 4720: Loss 0.02288738824427128\n",
            "Epoch 4730: Loss 0.022840864956378937\n",
            "Epoch 4740: Loss 0.022794539108872414\n",
            "Epoch 4750: Loss 0.022748423740267754\n",
            "Epoch 4760: Loss 0.022702500224113464\n",
            "Epoch 4770: Loss 0.022656762972474098\n",
            "Epoch 4780: Loss 0.022611239925026894\n",
            "Epoch 4790: Loss 0.022565895691514015\n",
            "Epoch 4800: Loss 0.02252076007425785\n",
            "Epoch 4810: Loss 0.02247580513358116\n",
            "Epoch 4820: Loss 0.022431042045354843\n",
            "Epoch 4830: Loss 0.022386470809578896\n",
            "Epoch 4840: Loss 0.022342078387737274\n",
            "Epoch 4850: Loss 0.02229788340628147\n",
            "Epoch 4860: Loss 0.022253870964050293\n",
            "Epoch 4870: Loss 0.02221004292368889\n",
            "Epoch 4880: Loss 0.022166399285197258\n",
            "Epoch 4890: Loss 0.022122932597994804\n",
            "Epoch 4900: Loss 0.02207965776324272\n",
            "Epoch 4910: Loss 0.022036539390683174\n",
            "Epoch 4920: Loss 0.021993624046444893\n",
            "Epoch 4930: Loss 0.021950874477624893\n",
            "Epoch 4940: Loss 0.02190830558538437\n",
            "Epoch 4950: Loss 0.021865908056497574\n",
            "Epoch 4960: Loss 0.021823693066835403\n",
            "Epoch 4970: Loss 0.021781638264656067\n",
            "Epoch 4980: Loss 0.021739766001701355\n",
            "Epoch 4990: Loss 0.021698055788874626\n",
            "UAR: 0.5911574552512664\n"
          ]
        }
      ],
      "source": [
        "model = BasicPerceptron(input_size=306, num_classes=8)\n",
        "model.train_model(x_train, y_train, num_epochs=5000, learning_rate=0.1)\n",
        "\n",
        "# 2. Predict labels for the testing/validation data\n",
        "y_pred = model.predict(x_test)\n",
        "\n",
        "# 3. Calculate accuracy\n",
        "uar = model.estimate(y_test, y_pred)\n",
        "\n",
        "print(f\"UAR: {uar}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoaufiM9FLOY"
      },
      "source": [
        "### TRY K-FOLD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "metadata": {
        "id": "Ww6hRbBw9Tv2"
      },
      "outputs": [],
      "source": [
        "def load_dataset(X_path='/content/feature_mfcc_34_delta_delta_nfft_8192.csv',\n",
        "                 y_path='/content/y_labels_feature_34_mfcc_delta_delta_nfft_8192.csv',\n",
        "                 ID_path='/content/IDs_feature_mfcc_34_delta_delta_nfft_8192.csv'):\n",
        "    starting_time = time.time()\n",
        "    X = pd.read_csv(X_path)\n",
        "    X = X.drop('Unnamed: 0', axis=1)\n",
        "    y = pd.read_csv(y_path)\n",
        "    y = y.drop('Unnamed: 0', axis=1)\n",
        "    ID = pd.read_csv(ID_path)\n",
        "    ID = ID.drop('Unnamed: 0', axis=1)\n",
        "\n",
        "    print(\"data loaded in \" + str(time.time() - starting_time) + \"ms\")\n",
        "    print(X.head())\n",
        "    print(\"X.shape = \", X.shape)\n",
        "    print(\"X.columns = \", X.columns)\n",
        "\n",
        "    return X, y, ID\n",
        "\n",
        "def get_k_fold_group_member():\n",
        "    return {\n",
        "        '0': {2, 5, 14, 15, 16},\n",
        "        '1': {3, 6, 7, 13, 18},\n",
        "        '2': {10, 11, 12, 19, 20},\n",
        "        '3': {8, 17, 21, 23, 24},\n",
        "        '4': {1, 4, 9, 22}\n",
        "    }\n",
        "\n",
        "def get_custom_k_folds(X, y, ID, group_members):\n",
        "    X_k_fold = dict()\n",
        "    y_k_fold = dict()\n",
        "    for k, members in group_members.items():\n",
        "        fold_X = pd.DataFrame()\n",
        "        fold_y = pd.DataFrame()\n",
        "\n",
        "        for actor_ID in members:\n",
        "            inds = ID[ID['0'] == actor_ID].index.tolist()\n",
        "            fold_X = pd.concat([fold_X, X.loc[inds, :]])\n",
        "            fold_y = pd.concat([fold_y, y.loc[inds, :]])\n",
        "        X_k_fold[k] = fold_X\n",
        "        y_k_fold[k] = fold_y\n",
        "    return X_k_fold, y_k_fold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 177,
      "metadata": {
        "id": "ANyAKl7o95yf"
      },
      "outputs": [],
      "source": [
        "# Load data from CSV files using pandas\n",
        "X = pd.read_csv('/content/feature_mfcc_34_delta_delta_nfft_8192.csv', index_col=0)\n",
        "y = pd.read_csv('/content/y_labels_feature_34_mfcc_delta_delta_nfft_8192.csv', index_col=0)\n",
        "IDs = pd.read_csv('/content/IDs_feature_mfcc_34_delta_delta_nfft_8192.csv', index_col=0)\n",
        "\n",
        "group_members= get_k_fold_group_member()\n",
        "\n",
        "\n",
        "X_k_folds, y_k_folds = get_custom_k_folds(X, y, IDs, group_members)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_hA2ZqZAikw",
        "outputId": "1dd99029-be3e-4515-bb1b-74a504365506"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'0':               0\n",
            "1380       calm\n",
            "1381       calm\n",
            "1382    neutral\n",
            "1383      angry\n",
            "1384  surprised\n",
            "...         ...\n",
            "595     disgust\n",
            "596         sad\n",
            "597         sad\n",
            "598       angry\n",
            "599   surprised\n",
            "\n",
            "[300 rows x 1 columns], '1':              0\n",
            "120       calm\n",
            "121      angry\n",
            "122      angry\n",
            "123        sad\n",
            "124  surprised\n",
            "..         ...\n",
            "955    disgust\n",
            "956       calm\n",
            "957      angry\n",
            "958        sad\n",
            "959        sad\n",
            "\n",
            "[300 rows x 1 columns], '2':               0\n",
            "1200      happy\n",
            "1201        sad\n",
            "1202    neutral\n",
            "1203    neutral\n",
            "1204    neutral\n",
            "...         ...\n",
            "475   surprised\n",
            "476       angry\n",
            "477       happy\n",
            "478   surprised\n",
            "479        calm\n",
            "\n",
            "[300 rows x 1 columns], '3':               0\n",
            "1320        sad\n",
            "1321       calm\n",
            "1322    disgust\n",
            "1323  surprised\n",
            "1324       calm\n",
            "...         ...\n",
            "295   surprised\n",
            "296   surprised\n",
            "297         sad\n",
            "298       angry\n",
            "299     neutral\n",
            "\n",
            "[300 rows x 1 columns], '4':               0\n",
            "1260      happy\n",
            "1261    disgust\n",
            "1262    disgust\n",
            "1263    disgust\n",
            "1264       fear\n",
            "...         ...\n",
            "55          sad\n",
            "56          sad\n",
            "57         calm\n",
            "58    surprised\n",
            "59         calm\n",
            "\n",
            "[240 rows x 1 columns]}\n"
          ]
        }
      ],
      "source": [
        "print(y_k_folds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "metadata": {
        "id": "2BPAsFSMACBW"
      },
      "outputs": [],
      "source": [
        "def estimate_model(model, X_k_folds, y_k_folds, num_epochs=10000):\n",
        "    y_pred = []\n",
        "    y_true = []\n",
        "\n",
        "    for k in X_k_folds.keys():\n",
        "        # Prepare dataset\n",
        "        X_train = pd.DataFrame()\n",
        "        y_train = pd.DataFrame()\n",
        "        for i in X_k_folds.keys():\n",
        "            if (i != k):\n",
        "                X_train = pd.concat([X_train, X_k_folds[i]])\n",
        "                y_train = pd.concat([y_train, y_k_folds[i]])\n",
        "            else:\n",
        "                X_test = X_k_folds[i]\n",
        "                y_test = y_k_folds[i]\n",
        "        x_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
        "        y_train_normalized = pd.Series(y_train.values.ravel()).replace({'neutral':0, 'calm':1, 'happy':2, 'sad':3, 'angry':4, 'fear':5, 'disgust':6, 'surprised':7})\n",
        "        y_tensor = torch.tensor(y_train_normalized)  # Assuming labels are integers (long tensor)\n",
        "        model.train_model(x_tensor, y_tensor)\n",
        "        y_pred_k_fold = model.predict(torch.tensor(X_test.values, dtype=torch.float32))\n",
        "\n",
        "        y_pred = np.concatenate((y_pred, y_pred_k_fold), axis=None)\n",
        "        y_true = np.concatenate((y_true, y_test.values), axis=None)\n",
        "\n",
        "    UAR = model.estimate(y_true, y_pred)\n",
        "    # print(f'UAR = {UAR:.3f}')\n",
        "    return UAR, y_pred, y_true"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 198,
      "metadata": {
        "id": "9VAjduWtAK4O"
      },
      "outputs": [],
      "source": [
        "model = BasicPerceptron(input_size=306, num_classes=8)\n",
        "uar, y_pred, y_true=estimate_model(model, X_k_folds, y_k_folds, num_epochs=10)\n",
        "\n",
        "print(f\"UAR: {uar}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7W540oJzl5E"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
